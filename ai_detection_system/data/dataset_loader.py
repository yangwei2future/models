#!/usr/bin/env python3
"""
æ•°æ®é›†åŠ è½½å™¨ - AIä»£ç æ£€æµ‹è®­ç»ƒæ•°æ®
ç”¨äºŽåŠ è½½å’Œå¤„ç†Human vs AIä»£ç æ ·æœ¬æ•°æ®é›†
"""

import os
import json
import logging
from typing import List, Dict, Tuple, Any, Optional
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CodeDatasetLoader:
    """AIä»£ç æ£€æµ‹æ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨
        
        Args:
            data_dir: æ•°æ®é›†ç›®å½•è·¯å¾„
        """
        self.data_dir = Path(data_dir)
        self.labels_file = self.data_dir / "dataset_labels.json"
        self.dataset_info = None
        self.labels_data = None
        
        # åŠ è½½æ ‡æ³¨æ•°æ®
        self._load_labels()
    
    def _load_labels(self):
        """åŠ è½½æ•°æ®é›†æ ‡æ³¨ä¿¡æ¯"""
        try:
            if self.labels_file.exists():
                with open(self.labels_file, 'r', encoding='utf-8') as f:
                    self.labels_data = json.load(f)
                    self.dataset_info = self.labels_data.get('dataset_info', {})
                logger.info(f"Loaded dataset labels: {self.dataset_info.get('description', 'Unknown')}")
            else:
                logger.warning(f"Labels file not found: {self.labels_file}")
                self.labels_data = {}
                self.dataset_info = {}
        except Exception as e:
            logger.error(f"Failed to load labels: {e}")
            self.labels_data = {}
            self.dataset_info = {}
    
    def load_file_samples(self, filename: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½å•ä¸ªæ–‡ä»¶çš„ä»£ç æ ·æœ¬
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            ä»£ç è¡Œæ ·æœ¬åˆ—è¡¨
        """
        file_path = self.data_dir / filename
        
        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return []
        
        try:
            # è¯»å–ä»£ç æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            samples = []
            file_labels = {}
            file_type = 'unknown'
            
            if self.labels_data and 'files' in self.labels_data:
                file_info = self.labels_data['files'].get(filename, {})
                file_labels = file_info.get('line_labels', {})
                file_type = file_info.get('file_type', 'unknown')
            
            for line_num, line_content in enumerate(lines, 1):
                line_content = line_content.rstrip('\n\r')
                
                # è·³è¿‡ç©ºè¡Œ
                if not line_content.strip():
                    continue
                
                # èŽ·å–æ ‡æ³¨ä¿¡æ¯
                line_key = str(line_num)
                if line_key in file_labels:
                    # ä½¿ç”¨æ ‡æ³¨çš„æ ‡ç­¾
                    label_info = file_labels[line_key]
                    is_ai = label_info.get('is_ai', False)
                    confidence = label_info.get('confidence', 0.5)
                else:
                    # ä½¿ç”¨æ–‡ä»¶ç±»åž‹ä½œä¸ºé»˜è®¤æ ‡ç­¾
                    if file_type == 'human':
                        is_ai = False
                        confidence = 0.8
                    elif file_type == 'ai':
                        is_ai = True
                        confidence = 0.8
                    else:
                        # æ··åˆæ–‡ä»¶ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™
                        is_ai = self._heuristic_ai_detection(line_content)
                        confidence = 0.6
                
                sample = {
                    'line': line_content,
                    'line_number': line_num,
                    'total_lines': len(lines),
                    'is_ai': is_ai,
                    'confidence': confidence,
                    'file_type': file_type,
                    'filename': filename
                }
                
                samples.append(sample)
            
            logger.info(f"Loaded {len(samples)} samples from {filename}")
            return samples
            
        except Exception as e:
            logger.error(f"Failed to load file {filename}: {e}")
            return []
    
    def _heuristic_ai_detection(self, line: str) -> bool:
        """
        å¯å‘å¼AIä»£ç æ£€æµ‹ï¼ˆç”¨äºŽæœªæ ‡æ³¨çš„è¡Œï¼‰
        
        Args:
            line: ä»£ç è¡Œå†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºAIç”Ÿæˆçš„ä»£ç 
        """
        ai_indicators = [
            '"""',  # è¯¦ç»†æ–‡æ¡£å­—ç¬¦ä¸²
            'Args:',  # å‚æ•°æ–‡æ¡£
            'Returns:',  # è¿”å›žå€¼æ–‡æ¡£
            'Raises:',  # å¼‚å¸¸æ–‡æ¡£
            'typing import',  # ç±»åž‹æ³¨è§£å¯¼å…¥
            'Optional[',  # å¯é€‰ç±»åž‹
            'Dict[str, Any]',  # å¤æ‚ç±»åž‹æ³¨è§£
            'comprehensive',  # AIå¸¸ç”¨è¯æ±‡
            'sophisticated',
            'advanced',
            'implementation',
            'configuration',
            'initialization',
            'processing',
            'logging.getLogger',  # è¯¦ç»†æ—¥å¿—é…ç½®
            'abstractmethod',  # æŠ½è±¡æ–¹æ³•
            '@dataclass',  # æ•°æ®ç±»è£…é¥°å™¨
        ]
        
        line_lower = line.lower()
        ai_score = sum(1 for indicator in ai_indicators if indicator.lower() in line_lower)
        
        # å¦‚æžœåŒ…å«2ä¸ªæˆ–ä»¥ä¸ŠAIæŒ‡æ ‡ï¼Œè®¤ä¸ºæ˜¯AIç”Ÿæˆ
        return ai_score >= 2
    
    def load_all_samples(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ‰€æœ‰ä»£ç æ ·æœ¬
        
        Returns:
            æ‰€æœ‰ä»£ç è¡Œæ ·æœ¬åˆ—è¡¨
        """
        all_samples = []
        
        # èŽ·å–æ‰€æœ‰Pythonæ–‡ä»¶
        python_files = [
            'human_code_samples.py',
            'ai_code_samples.py',
            'mixed_code_samples.py'
        ]
        
        for filename in python_files:
            samples = self.load_file_samples(filename)
            all_samples.extend(samples)
        
        logger.info(f"Total samples loaded: {len(all_samples)}")
        return all_samples
    
    def get_balanced_dataset(self, max_samples_per_class: Optional[int] = None) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        èŽ·å–å¹³è¡¡çš„æ•°æ®é›†
        
        Args:
            max_samples_per_class: æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•°
            
        Returns:
            (human_samples, ai_samples) å…ƒç»„
        """
        all_samples = self.load_all_samples()
        
        # åˆ†ç¦»Humanå’ŒAIæ ·æœ¬
        human_samples = [s for s in all_samples if not s['is_ai']]
        ai_samples = [s for s in all_samples if s['is_ai']]
        
        logger.info(f"Human samples: {len(human_samples)}, AI samples: {len(ai_samples)}")
        
        # å¹³è¡¡æ•°æ®é›†
        if max_samples_per_class:
            human_samples = human_samples[:max_samples_per_class]
            ai_samples = ai_samples[:max_samples_per_class]
        else:
            # ä½¿ç”¨è¾ƒå°ç±»åˆ«çš„å¤§å°
            min_size = min(len(human_samples), len(ai_samples))
            human_samples = human_samples[:min_size]
            ai_samples = ai_samples[:min_size]
        
        logger.info(f"Balanced dataset - Human: {len(human_samples)}, AI: {len(ai_samples)}")
        
        return human_samples, ai_samples
    
    def get_training_data(self, test_split: float = 0.2) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        èŽ·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        
        Args:
            test_split: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            (train_samples, test_samples) å…ƒç»„
        """
        human_samples, ai_samples = self.get_balanced_dataset()
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        human_split = int(len(human_samples) * (1 - test_split))
        ai_split = int(len(ai_samples) * (1 - test_split))
        
        # åˆ†å‰²æ•°æ®
        train_samples = human_samples[:human_split] + ai_samples[:ai_split]
        test_samples = human_samples[human_split:] + ai_samples[ai_split:]
        
        # æ‰“ä¹±é¡ºåº
        import random
        random.shuffle(train_samples)
        random.shuffle(test_samples)
        
        logger.info(f"Training samples: {len(train_samples)}, Test samples: {len(test_samples)}")
        
        return train_samples, test_samples
    
    def export_training_data(self, output_file: str = "training_data.json"):
        """
        å¯¼å‡ºè®­ç»ƒæ•°æ®ä¸ºJSONæ ¼å¼
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶å
        """
        train_samples, test_samples = self.get_training_data()
        
        export_data = {
            'dataset_info': self.dataset_info,
            'training_data': train_samples,
            'test_data': test_samples,
            'statistics': {
                'total_training_samples': len(train_samples),
                'total_test_samples': len(test_samples),
                'training_human_samples': sum(1 for s in train_samples if not s['is_ai']),
                'training_ai_samples': sum(1 for s in train_samples if s['is_ai']),
                'test_human_samples': sum(1 for s in test_samples if not s['is_ai']),
                'test_ai_samples': sum(1 for s in test_samples if s['is_ai'])
            }
        }
        
        output_path = self.data_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Training data exported to {output_path}")
        
        return export_data
    
    def get_dataset_statistics(self) -> Dict[str, Any]:
        """
        èŽ·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        all_samples = self.load_all_samples()
        
        human_samples = [s for s in all_samples if not s['is_ai']]
        ai_samples = [s for s in all_samples if s['is_ai']]
        
        # æŒ‰æ–‡ä»¶ç±»åž‹ç»Ÿè®¡
        file_stats = {}
        for sample in all_samples:
            filename = sample['filename']
            if filename not in file_stats:
                file_stats[filename] = {'total': 0, 'human': 0, 'ai': 0}
            
            file_stats[filename]['total'] += 1
            if sample['is_ai']:
                file_stats[filename]['ai'] += 1
            else:
                file_stats[filename]['human'] += 1
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence_levels = {
            'high': sum(1 for s in all_samples if s['confidence'] >= 0.8),
            'medium': sum(1 for s in all_samples if 0.6 <= s['confidence'] < 0.8),
            'low': sum(1 for s in all_samples if s['confidence'] < 0.6)
        }
        
        statistics = {
            'total_samples': len(all_samples),
            'human_samples': len(human_samples),
            'ai_samples': len(ai_samples),
            'human_percentage': len(human_samples) / len(all_samples) * 100 if all_samples else 0,
            'ai_percentage': len(ai_samples) / len(all_samples) * 100 if all_samples else 0,
            'file_statistics': file_stats,
            'confidence_distribution': confidence_levels,
            'average_confidence': sum(s['confidence'] for s in all_samples) / len(all_samples) if all_samples else 0
        }
        
        return statistics


def main():
    """æ¼”ç¤ºæ•°æ®é›†åŠ è½½å™¨çš„ä½¿ç”¨"""
    logger.info("ðŸ—‚ï¸  AI Code Detection Dataset Loader Demo")
    logger.info("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨
    loader = CodeDatasetLoader()
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    stats = loader.get_dataset_statistics()
    logger.info("ðŸ“Š Dataset Statistics:")
    logger.info(f"  Total samples: {stats['total_samples']}")
    logger.info(f"  Human samples: {stats['human_samples']} ({stats['human_percentage']:.1f}%)")
    logger.info(f"  AI samples: {stats['ai_samples']} ({stats['ai_percentage']:.1f}%)")
    logger.info(f"  Average confidence: {stats['average_confidence']:.3f}")
    
    logger.info("\nðŸ“ File Statistics:")
    for filename, file_stat in stats['file_statistics'].items():
        logger.info(f"  {filename}: {file_stat['total']} total ({file_stat['human']} human, {file_stat['ai']} AI)")
    
    # èŽ·å–å¹³è¡¡æ•°æ®é›†
    human_samples, ai_samples = loader.get_balanced_dataset(max_samples_per_class=50)
    logger.info(f"\nâš–ï¸  Balanced Dataset: {len(human_samples)} human, {len(ai_samples)} AI")
    
    # èŽ·å–è®­ç»ƒæ•°æ®
    train_samples, test_samples = loader.get_training_data(test_split=0.2)
    logger.info(f"\nðŸš‚ Training Data: {len(train_samples)} train, {len(test_samples)} test")
    
    # å¯¼å‡ºè®­ç»ƒæ•°æ®
    export_data = loader.export_training_data()
    logger.info(f"\nðŸ’¾ Exported training data with {export_data['statistics']['total_training_samples']} training samples")
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    logger.info("\nðŸ“ Sample Data:")
    for i, sample in enumerate(train_samples[:3]):
        label = "AI" if sample['is_ai'] else "Human"
        logger.info(f"  Sample {i+1} ({label}, conf: {sample['confidence']:.2f}): {sample['line'][:60]}...")
    
    logger.info("\nâœ… Dataset loader demo completed!")


if __name__ == "__main__":
    main() 