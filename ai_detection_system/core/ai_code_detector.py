#!/usr/bin/env python3
"""
AIä»£ç æ£€æµ‹å™¨ - åŸºäºCodeBERTçš„é€è¡Œæ£€æµ‹
ä½¿ç”¨CodeBERTç¼–ç å™¨ + ç‰¹å¾èåˆ + åˆ†ç±»å™¨æ¶æ„

ä½¿ç”¨æ–¹æ³•:
    python ai_code_detector.py --model models/ai_detector.pt --input file.py
    python ai_code_detector.py --model models/ai_detector.pt --input src/ --recursive
"""

import os
import sys
import json
import argparse
import logging
import glob
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from transformers import AutoTokenizer, AutoModel

# æ·»åŠ è·¯å¾„
sys.path.append('parsers/fileParser')
sys.path.append('.')

def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    return logging.getLogger(__name__)

class CodeBERTAIDetector(nn.Module):
    """åŸºäºCodeBERTçš„AIä»£ç æ£€æµ‹å™¨ - é€è¡Œæ£€æµ‹"""
    
    def __init__(self, model_name: str = "microsoft/codebert-base", feature_dim: int = 10, hidden_dim: int = 256):
        super().__init__()
        self.model_name = model_name
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        
        # CodeBERT tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.codebert = AutoModel.from_pretrained(model_name)
        
        # å†»ç»“CodeBERTçš„éƒ¨åˆ†å‚æ•°ï¼ˆå¯é€‰ï¼‰
        # for param in self.codebert.parameters():
        #     param.requires_grad = False
        
        # ç‰¹å¾ç»´åº¦
        self.codebert_dim = self.codebert.config.hidden_size  # 768 for base model
        
        # ç‰¹å¾èåˆå±‚
        self.feature_projection = nn.Linear(feature_dim, hidden_dim)
        self.code_projection = nn.Linear(self.codebert_dim, hidden_dim)
        
        # èåˆåçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),  # ç‰¹å¾ + CodeBERTè¾“å‡º
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1),  # è¾“å‡ºAIæ¦‚ç‡
            nn.Sigmoid()
        )
    
    def extract_line_features(self, line: str, line_number: int, total_lines: int) -> List[float]:
        """æå–è¡Œçº§ç‰¹å¾"""
        line = line.strip()
        
        # åŸºç¡€ç‰¹å¾
        length = len(line)
        indent_level = len(line) - len(line.lstrip()) if line else 0
        
        # å†…å®¹ç‰¹å¾
        has_comment = '#' in line or '//' in line or '/*' in line
        has_string = '"' in line or "'" in line or '`' in line
        has_number = any(c.isdigit() for c in line)
        has_operator = any(op in line for op in ['=', '+', '-', '*', '/', '<', '>', '!', '&', '|'])
        
        # ä½ç½®ç‰¹å¾
        relative_position = line_number / total_lines if total_lines > 0 else 0
        
        # AIç”Ÿæˆä»£ç çš„å¸¸è§æ¨¡å¼
        ai_patterns = [
            'generate', 'create', 'implement', 'function', 'method',
            'algorithm', 'process', 'handle', 'manage', 'execute',
            'calculate', 'compute', 'optimize', 'initialize', 'configure'
        ]
        has_ai_pattern = any(pattern in line.lower() for pattern in ai_patterns)
        
        # å¤æ‚åº¦å’Œé£æ ¼ç‰¹å¾
        complexity = line.count('(') + line.count('[') + line.count('{')
        word_density = len(line.split()) / max(len(line), 1)
        
        return [
            length / 100.0,  # å½’ä¸€åŒ–é•¿åº¦
            indent_level / 20.0,  # å½’ä¸€åŒ–ç¼©è¿›
            float(has_comment),
            float(has_string),
            float(has_number),
            float(has_operator),
            relative_position,
            float(has_ai_pattern),
            complexity / 10.0,
            word_density
        ]
    
    def encode_with_codebert(self, line: str) -> torch.Tensor:
        """ä½¿ç”¨CodeBERTç¼–ç ä»£ç è¡Œ"""
        # å¤„ç†ç©ºè¡Œ
        if not line.strip():
            line = "[EMPTY_LINE]"
        
        # Tokenize
        inputs = self.tokenizer(
            line,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = next(self.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # CodeBERTç¼–ç 
        with torch.no_grad():
            outputs = self.codebert(**inputs)
        
        # ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
        code_embedding = outputs.last_hidden_state[:, 0, :]  # [1, hidden_size]
        
        return code_embedding
    
    def forward(self, line: str, line_number: int, total_lines: int) -> float:
        """å‰å‘ä¼ æ’­ - é¢„æµ‹å•è¡ŒAIæ¦‚ç‡"""
        device = next(self.parameters()).device
        
        # 1. æå–è¡Œçº§ç‰¹å¾
        line_features = self.extract_line_features(line, line_number, total_lines)
        line_features_tensor = torch.tensor(line_features, device=device, dtype=torch.float32).unsqueeze(0)
        
        # 2. CodeBERTç¼–ç 
        code_embedding = self.encode_with_codebert(line)
        
        # 3. ç‰¹å¾æŠ•å½±
        projected_features = self.feature_projection(line_features_tensor)  # [1, hidden_dim]
        projected_code = self.code_projection(code_embedding)  # [1, hidden_dim]
        
        # 4. ç‰¹å¾èåˆ
        fused_features = torch.cat([projected_features, projected_code], dim=1)  # [1, hidden_dim*2]
        
        # 5. åˆ†ç±»é¢„æµ‹
        ai_prob = self.classifier(fused_features).item()
        
        return ai_prob

class AICodeDetector:
    """AIä»£ç æ£€æµ‹å™¨ä¸»ç±»"""
    
    def __init__(self, model_path: str, threshold: float = 0.5):
        self.model_path = model_path
        self.threshold = threshold
        self.logger = logging.getLogger(__name__)
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        self.supported_extensions = {
            '.py', '.java', '.js', '.jsx', '.ts', '.tsx',
            '.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.hxx',
            '.go', '.rs', '.rb', '.php', '.swift', '.kt'
        }
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = CodeBERTAIDetector()
            
            if os.path.exists(self.model_path):
                checkpoint = torch.load(self.model_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    self.model.load_state_dict(checkpoint)
                self.logger.info(f"âœ… CodeBERT model loaded from {self.model_path}")
            else:
                self.logger.warning(f"Model file not found: {self.model_path}, using random weights")
            
            self.model.eval()
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            raise
    
    def detect_file(self, filepath: str) -> Dict[str, Any]:
        """æ£€æµ‹å•ä¸ªæ–‡ä»¶çš„æ¯ä¸€è¡Œ"""
        try:
            # è¯»å–æ–‡ä»¶
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            if not lines:
                return {
                    "file_path": filepath,
                    "success": False,
                    "error": "Empty file"
                }
            
            # é€è¡Œæ£€æµ‹
            line_results = []
            total_lines = len(lines)
            
            for i, line in enumerate(lines, 1):
                content = line.rstrip('\n\r')
                
                # è·³è¿‡ç©ºè¡Œ
                if not content.strip():
                    line_results.append({
                        "line_number": i,
                        "content": content,
                        "ai_prob": 0.0,
                        "is_ai": False,
                        "type": "empty"
                    })
                    continue
                
                # AIæ¦‚ç‡é¢„æµ‹
                with torch.no_grad():
                    ai_prob = self.model(content, i, total_lines)
                
                is_ai = ai_prob > self.threshold
                
                line_results.append({
                    "line_number": i,
                    "content": content,
                    "ai_prob": round(ai_prob, 3),
                    "is_ai": is_ai,
                    "type": "code" if content.strip() else "empty"
                })
            
            # ç»Ÿè®¡ä¿¡æ¯
            ai_lines = sum(1 for r in line_results if r["is_ai"])
            total_code_lines = sum(1 for r in line_results if r["type"] == "code")
            
            return {
                "file_path": filepath,
                "success": True,
                "lines": line_results,
                "summary": {
                    "total_lines": total_lines,
                    "code_lines": total_code_lines,
                    "ai_lines": ai_lines,
                    "ai_percentage": round((ai_lines / total_code_lines * 100) if total_code_lines > 0 else 0, 1),
                    "average_ai_prob": round(np.mean([r["ai_prob"] for r in line_results if r["type"] == "code"]), 3) if total_code_lines > 0 else 0.0
                }
            }
            
        except Exception as e:
            return {
                "file_path": filepath,
                "success": False,
                "error": str(e)
            }
    
    def is_code_file(self, filepath: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä»£ç æ–‡ä»¶"""
        _, ext = os.path.splitext(filepath.lower())
        return ext in self.supported_extensions
    
    def collect_files(self, input_paths: List[str], recursive: bool = False) -> List[str]:
        """æ”¶é›†è¦å¤„ç†çš„æ–‡ä»¶"""
        files = []
        
        for input_path in input_paths:
            # å¤„ç†é€šé…ç¬¦
            if '*' in input_path or '?' in input_path:
                matched_files = glob.glob(input_path)
                files.extend([f for f in matched_files if os.path.isfile(f) and self.is_code_file(f)])
                continue
            
            if os.path.isfile(input_path):
                if self.is_code_file(input_path):
                    files.append(input_path)
                else:
                    self.logger.warning(f"Skipping unsupported file: {input_path}")
            
            elif os.path.isdir(input_path):
                if recursive:
                    for root, dirs, filenames in os.walk(input_path):
                        for filename in filenames:
                            filepath = os.path.join(root, filename)
                            if self.is_code_file(filepath):
                                files.append(filepath)
                else:
                    for filename in os.listdir(input_path):
                        filepath = os.path.join(input_path, filename)
                        if os.path.isfile(filepath) and self.is_code_file(filepath):
                            files.append(filepath)
            else:
                self.logger.warning(f"Path not found: {input_path}")
        
        return sorted(list(set(files)))
    
    def detect_batch(self, filepaths: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡æ£€æµ‹å¤šä¸ªæ–‡ä»¶"""
        results = []
        stats = {
            "total_files": len(filepaths),
            "successful_files": 0,
            "failed_files": 0,
            "total_lines": 0,
            "total_ai_lines": 0,
            "file_ai_percentages": []
        }
        
        self.logger.info(f"Processing {len(filepaths)} files...")
        
        for i, filepath in enumerate(filepaths, 1):
            self.logger.debug(f"Processing {i}/{len(filepaths)}: {filepath}")
            
            result = self.detect_file(filepath)
            results.append(result)
            
            if result["success"]:
                stats["successful_files"] += 1
                summary = result["summary"]
                stats["total_lines"] += summary["total_lines"]
                stats["total_ai_lines"] += summary["ai_lines"]
                stats["file_ai_percentages"].append(summary["ai_percentage"])
                
                self.logger.info(f"âœ… {os.path.basename(filepath)} -> {summary['ai_percentage']}% AI ({summary['ai_lines']}/{summary['code_lines']} lines)")
            else:
                stats["failed_files"] += 1
                self.logger.warning(f"âŒ {os.path.basename(filepath)} -> {result['error']}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if stats["successful_files"] > 0:
            stats["overall_ai_percentage"] = round((stats["total_ai_lines"] / (stats["total_lines"] - sum(r["summary"]["total_lines"] - r["summary"]["code_lines"] for r in results if r["success"])) * 100) if stats["total_lines"] > 0 else 0, 1)
            stats["average_file_ai_percentage"] = round(np.mean(stats["file_ai_percentages"]), 1)
        else:
            stats["overall_ai_percentage"] = 0.0
            stats["average_file_ai_percentage"] = 0.0
        
        return {
            "results": results,
            "statistics": stats,
            "metadata": {
                "model_path": self.model_path,
                "model_type": "CodeBERT-based",
                "threshold": self.threshold,
                "timestamp": datetime.now().isoformat()
            }
        }

def print_results(detection_results: Dict[str, Any], verbose: bool = False, show_lines: bool = False):
    """æ‰“å°æ£€æµ‹ç»“æœ"""
    stats = detection_results["statistics"]
    
    print("\n" + "=" * 80)
    print("ğŸ¤– AI CODE DETECTION RESULTS (CodeBERT-based)")
    print("=" * 80)
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“Š Summary:")
    print(f"   Total files: {stats['total_files']}")
    print(f"   Successful: {stats['successful_files']}")
    print(f"   Failed: {stats['failed_files']}")
    print(f"   Total lines: {stats['total_lines']}")
    print(f"   AI-generated lines: {stats['total_ai_lines']}")
    print(f"   Overall AI percentage: {stats['overall_ai_percentage']}%")
    print(f"   Average per file: {stats['average_file_ai_percentage']}%")
    
    # è¯¦ç»†ç»“æœ
    if verbose or show_lines:
        print(f"\nğŸ“„ Detailed Results:")
        print("-" * 80)
        
        for result in detection_results["results"]:
            if not result["success"]:
                print(f"\nâŒ {result['file_path']}")
                print(f"   Error: {result['error']}")
                continue
            
            summary = result["summary"]
            print(f"\nğŸ“ {result['file_path']}")
            print(f"   Lines: {summary['total_lines']} total, {summary['code_lines']} code")
            print(f"   AI detection: {summary['ai_lines']} lines ({summary['ai_percentage']}%)")
            print(f"   Average AI probability: {summary['average_ai_prob']}")
            
            # æ˜¾ç¤ºé€è¡Œç»“æœ
            if show_lines:
                print("   Line-by-line analysis:")
                for line_result in result["lines"]:
                    if line_result["type"] == "code" and line_result["ai_prob"] > 0.3:  # åªæ˜¾ç¤ºå¯èƒ½çš„AIè¡Œ
                        status = "ğŸ¤– AI" if line_result["is_ai"] else "ğŸ‘¤ Human"
                        print(f"     {line_result['line_number']:3d}: {status} ({line_result['ai_prob']:.3f}) | {line_result['content'][:60]}")

def main():
    parser = argparse.ArgumentParser(
        description="AI Code Detection Tool - CodeBERT-based line-by-line detection",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --model models/ai_detector.pt --input file.py
  %(prog)s --model models/ai_detector.pt --input src/ --recursive
  %(prog)s --model models/ai_detector.pt --input "*.py" --output results.json --show-lines
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--model", type=str, required=True,
                       help="Path to AI detection model file (.pt)")
    parser.add_argument("--input", type=str, nargs='+', required=True,
                       help="Input files, directories, or patterns to process")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--output", type=str, default=None,
                       help="Output JSON file to save results")
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="AI detection threshold (0.0-1.0, default: 0.5)")
    parser.add_argument("--recursive", "-r", action="store_true",
                       help="Recursively process subdirectories")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Show detailed results")
    parser.add_argument("--show-lines", action="store_true",
                       help="Show line-by-line AI detection results")
    parser.add_argument("--quiet", "-q", action="store_true",
                       help="Suppress progress messages")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.verbose and not args.quiet)
    
    if not args.quiet:
        print("ğŸ¤– AI Code Detection Tool (CodeBERT-based)")
        print("-" * 50)
        print(f"Model: {args.model}")
        print(f"Input: {args.input}")
        print(f"Threshold: {args.threshold}")
        print(f"Recursive: {args.recursive}")
    
    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        detector = AICodeDetector(args.model, args.threshold)
        
        # æ”¶é›†æ–‡ä»¶
        files = detector.collect_files(args.input, args.recursive)
        
        if not files:
            logger.error("No supported code files found to process")
            return 1
        
        if not args.quiet:
            logger.info(f"Found {len(files)} code files to process")
        
        # æ‰§è¡Œæ£€æµ‹
        results = detector.detect_batch(files)
        
        # æ˜¾ç¤ºç»“æœ
        if not args.quiet:
            print_results(results, args.verbose, args.show_lines)
        
        # ä¿å­˜ç»“æœ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Results saved to {args.output}")
        
        # è¿”å›çŠ¶æ€ç 
        stats = results["statistics"]
        if stats["failed_files"] > 0 and stats["successful_files"] == 0:
            return 1  # å…¨éƒ¨å¤±è´¥
        else:
            return 0  # æˆåŠŸ
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸  Processing interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main()) 