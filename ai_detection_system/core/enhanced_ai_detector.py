#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆAIä»£ç æ£€æµ‹å™¨ - å®Œæ•´æ¶æ„å®ç°
æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„æµç¨‹å›¾å®ç°ï¼šæ–‡ä»¶è§£æ â†’ ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ CodeBERT â†’ åˆ†ç±»å™¨ â†’ é˜ˆå€¼è¿‡æ»¤ â†’ ç»“æœèšåˆ

æ¶æ„æµç¨‹:
ä»£ç æ–‡ä»¶/æ–‡ä»¶å¤¹ â†’ æ–‡ä»¶è§£æå™¨ â†’ è¡Œçº§ç‰¹å¾æå– â†’ æ–‡ä»¶çº§ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ 
è¡Œé—´å…³ç³»å»ºæ¨¡ â†’ è¡Œåˆ†ç±»å™¨ â†’ é¢„æµ‹æ¦‚ç‡ â†’ é˜ˆå€¼è¿‡æ»¤ â†’ AIç”Ÿæˆ/äººå·¥ç¼–ç  â†’ ç»“æœèšåˆ â†’ è¾“å‡ºç³»ç»Ÿ
"""

import os
import sys
import json
import argparse
import logging
import glob
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from transformers import AutoTokenizer, AutoModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FileParser:
    """æ–‡ä»¶è§£æå™¨ - è§£æä»£ç æ–‡ä»¶å¹¶æå–åŸºç¡€ä¿¡æ¯"""
    
    def __init__(self):
        self.supported_extensions = {
            '.py', '.java', '.js', '.jsx', '.ts', '.tsx',
            '.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.hxx',
            '.go', '.rs', '.rb', '.php', '.swift', '.kt'
        }
    
    def parse_file(self, filepath: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            # åŸºç¡€æ–‡ä»¶ä¿¡æ¯
            file_info = {
                'filepath': filepath,
                'filename': os.path.basename(filepath),
                'extension': os.path.splitext(filepath)[1],
                'total_lines': len(lines),
                'lines': [],
                'success': True
            }
            
            # é€è¡Œè§£æ
            for i, line in enumerate(lines, 1):
                content = line.rstrip('\n\r')
                line_info = {
                    'line_number': i,
                    'content': content,
                    'original_content': line,
                    'is_empty': not content.strip(),
                    'indent_level': len(content) - len(content.lstrip()) if content else 0
                }
                file_info['lines'].append(line_info)
            
            return file_info
            
        except Exception as e:
            return {
                'filepath': filepath,
                'success': False,
                'error': str(e)
            }
    
    def parse_directory(self, dirpath: str, recursive: bool = False) -> List[Dict[str, Any]]:
        """è§£æç›®å½•ä¸­çš„æ‰€æœ‰ä»£ç æ–‡ä»¶"""
        files = []
        
        if recursive:
            for root, _, filenames in os.walk(dirpath):
                for filename in filenames:
                    if self.is_code_file(filename):
                        filepath = os.path.join(root, filename)
                        files.append(self.parse_file(filepath))
        else:
            for filename in os.listdir(dirpath):
                filepath = os.path.join(dirpath, filename)
                if os.path.isfile(filepath) and self.is_code_file(filename):
                    files.append(self.parse_file(filepath))
        
        return files
    
    def is_code_file(self, filename: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ”¯æŒçš„ä»£ç æ–‡ä»¶"""
        _, ext = os.path.splitext(filename.lower())
        return ext in self.supported_extensions


class LineFeatureExtractor:
    """è¡Œçº§ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        # AIç”Ÿæˆä»£ç çš„æ¨¡å¼
        self.ai_patterns = [
            'comprehensive', 'sophisticated', 'advanced', 'implementation',
            'configuration', 'initialization', 'processing', 'algorithm',
            'calculate', 'compute', 'optimize', 'generate', 'create',
            'implement', 'function', 'method', 'handle', 'manage', 'execute'
        ]
        
        # ä»£ç é£æ ¼æŒ‡æ ‡
        self.style_indicators = {
            'docstring_start': ['"""', "'''", '/**'],
            'type_annotations': [':', '->', 'List[', 'Dict[', 'Optional[', 'Union['],
            'logging_patterns': ['logger.', 'logging.', 'log.'],
            'exception_patterns': ['raise', 'except', 'try:', 'finally:'],
            'import_patterns': ['from typing import', 'from abc import', 'import logging']
        }
    
    def extract_features(self, line_info: Dict[str, Any], file_context: Dict[str, Any]) -> List[float]:
        """æå–å•è¡Œçš„ç‰¹å¾å‘é‡"""
        content = line_info['content']
        line_number = line_info['line_number']
        total_lines = file_context['total_lines']
        
        # åŸºç¡€ç‰¹å¾ (8ç»´)
        length = len(content)
        indent_level = line_info['indent_level']
        relative_position = line_number / total_lines if total_lines > 0 else 0
        
        # å†…å®¹ç‰¹å¾ (6ç»´)
        has_comment = '#' in content or '//' in content or '/*' in content
        has_string = '"' in content or "'" in content or '`' in content
        has_number = any(c.isdigit() for c in content)
        has_operator = any(op in content for op in ['=', '+', '-', '*', '/', '<', '>', '!', '&', '|'])
        
        # å¤æ‚åº¦ç‰¹å¾ (4ç»´)
        bracket_complexity = content.count('(') + content.count('[') + content.count('{')
        word_count = len(content.split())
        word_density = word_count / max(len(content), 1)
        char_diversity = len(set(content)) / max(len(content), 1)
        
        # AIæ¨¡å¼ç‰¹å¾ (5ç»´)
        has_ai_pattern = any(pattern in content.lower() for pattern in self.ai_patterns)
        has_docstring = any(pattern in content for pattern in self.style_indicators['docstring_start'])
        has_type_annotation = any(pattern in content for pattern in self.style_indicators['type_annotations'])
        has_logging = any(pattern in content for pattern in self.style_indicators['logging_patterns'])
        has_advanced_import = any(pattern in content for pattern in self.style_indicators['import_patterns'])
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾ (3ç»´)
        is_first_line = line_number == 1
        is_last_line = line_number == total_lines
        is_middle_section = 0.3 < relative_position < 0.7
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = [
            # åŸºç¡€ç‰¹å¾ (8ç»´)
            min(length / 100.0, 1.0),           # 0. å½’ä¸€åŒ–é•¿åº¦
            min(indent_level / 20.0, 1.0),      # 1. å½’ä¸€åŒ–ç¼©è¿›
            relative_position,                   # 2. ç›¸å¯¹ä½ç½®
            min(word_count / 20.0, 1.0),        # 3. è¯æ±‡æ•°é‡
            
            # å†…å®¹ç‰¹å¾ (6ç»´)
            float(has_comment),                  # 4. æ³¨é‡Š
            float(has_string),                   # 5. å­—ç¬¦ä¸²
            float(has_number),                   # 6. æ•°å­—
            float(has_operator),                 # 7. æ“ä½œç¬¦
            
            # å¤æ‚åº¦ç‰¹å¾ (4ç»´)
            min(bracket_complexity / 10.0, 1.0), # 8. æ‹¬å·å¤æ‚åº¦
            word_density,                        # 9. è¯æ±‡å¯†åº¦
            char_diversity,                      # 10. å­—ç¬¦å¤šæ ·æ€§
            
            # AIæ¨¡å¼ç‰¹å¾ (5ç»´)
            float(has_ai_pattern),               # 11. AIæ¨¡å¼
            float(has_docstring),                # 12. æ–‡æ¡£å­—ç¬¦ä¸²
            float(has_type_annotation),          # 13. ç±»å‹æ³¨è§£
            float(has_logging),                  # 14. æ—¥å¿—è®°å½•
            float(has_advanced_import),          # 15. é«˜çº§å¯¼å…¥
            
            # ä¸Šä¸‹æ–‡ç‰¹å¾ (3ç»´)
            float(is_first_line),                # 16. é¦–è¡Œ
            float(is_last_line),                 # 17. æœ«è¡Œ
            float(is_middle_section),            # 18. ä¸­é—´æ®µ
        ]
        
        return features


class FileFeatureExtractor:
    """æ–‡ä»¶çº§ç‰¹å¾æå–å™¨"""
    
    def extract_features(self, file_info: Dict[str, Any]) -> List[float]:
        """æå–æ–‡ä»¶çº§ç‰¹å¾"""
        lines = file_info['lines']
        total_lines = len(lines)
        
        if total_lines == 0:
            return [0.0] * 15  # è¿”å›15ç»´é›¶å‘é‡
        
        # ç»Ÿè®¡ç‰¹å¾
        non_empty_lines = [line for line in lines if not line['is_empty']]
        comment_lines = [line for line in non_empty_lines if '#' in line['content'] or '//' in line['content']]
        
        # åŸºç¡€ç»Ÿè®¡ (5ç»´)
        file_size = total_lines
        code_density = len(non_empty_lines) / total_lines
        comment_ratio = len(comment_lines) / max(len(non_empty_lines), 1)
        avg_line_length = np.mean([len(line['content']) for line in non_empty_lines]) if non_empty_lines else 0
        avg_indent = np.mean([line['indent_level'] for line in non_empty_lines]) if non_empty_lines else 0
        
        # å¤æ‚åº¦ç‰¹å¾ (5ç»´)
        total_brackets = sum(line['content'].count('(') + line['content'].count('[') + line['content'].count('{') 
                           for line in non_empty_lines)
        avg_complexity = total_brackets / max(len(non_empty_lines), 1)
        
        function_count = sum(1 for line in non_empty_lines if 'def ' in line['content'] or 'function ' in line['content'])
        class_count = sum(1 for line in non_empty_lines if 'class ' in line['content'])
        import_count = sum(1 for line in non_empty_lines if line['content'].strip().startswith(('import ', 'from ')))
        
        # AIé£æ ¼ç‰¹å¾ (5ç»´)
        docstring_count = sum(1 for line in non_empty_lines if '"""' in line['content'] or "'''" in line['content'])
        type_annotation_count = sum(1 for line in non_empty_lines if '->' in line['content'] or ': ' in line['content'])
        logging_count = sum(1 for line in non_empty_lines if 'logger' in line['content'] or 'logging' in line['content'])
        exception_count = sum(1 for line in non_empty_lines if any(keyword in line['content'] for keyword in ['try:', 'except', 'raise', 'finally:']))
        advanced_pattern_count = sum(1 for line in non_empty_lines if any(pattern in line['content'].lower() 
                                   for pattern in ['comprehensive', 'sophisticated', 'implementation', 'configuration']))
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = [
            # åŸºç¡€ç»Ÿè®¡ (5ç»´)
            min(file_size / 1000.0, 1.0),           # 0. æ–‡ä»¶å¤§å°
            code_density,                            # 1. ä»£ç å¯†åº¦
            comment_ratio,                           # 2. æ³¨é‡Šæ¯”ä¾‹
            min(avg_line_length / 80.0, 1.0),       # 3. å¹³å‡è¡Œé•¿åº¦
            min(avg_indent / 10.0, 1.0),            # 4. å¹³å‡ç¼©è¿›
            
            # å¤æ‚åº¦ç‰¹å¾ (5ç»´)
            min(avg_complexity / 5.0, 1.0),         # 5. å¹³å‡å¤æ‚åº¦
            min(function_count / 20.0, 1.0),        # 6. å‡½æ•°æ•°é‡
            min(class_count / 10.0, 1.0),           # 7. ç±»æ•°é‡
            min(import_count / 20.0, 1.0),          # 8. å¯¼å…¥æ•°é‡
            
            # AIé£æ ¼ç‰¹å¾ (5ç»´)
            min(docstring_count / 10.0, 1.0),       # 9. æ–‡æ¡£å­—ç¬¦ä¸²
            min(type_annotation_count / 20.0, 1.0), # 10. ç±»å‹æ³¨è§£
            min(logging_count / 10.0, 1.0),         # 11. æ—¥å¿—è®°å½•
            min(exception_count / 10.0, 1.0),       # 12. å¼‚å¸¸å¤„ç†
            min(advanced_pattern_count / 10.0, 1.0) # 13. é«˜çº§æ¨¡å¼
        ]
        
        return features


class FeatureFusion(nn.Module):
    """ç‰¹å¾èåˆæ¨¡å—"""
    
    def __init__(self, line_feature_dim: int = 19, file_feature_dim: int = 14, output_dim: int = 128):
        super().__init__()
        self.line_feature_dim = line_feature_dim
        self.file_feature_dim = file_feature_dim
        self.output_dim = output_dim
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.line_projection = nn.Linear(line_feature_dim, output_dim // 2)
        self.file_projection = nn.Linear(file_feature_dim, output_dim // 2)
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(output_dim)
        )
    
    def forward(self, line_features: torch.Tensor, file_features: torch.Tensor) -> torch.Tensor:
        """ç‰¹å¾èåˆå‰å‘ä¼ æ’­"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        line_proj = self.line_projection(line_features)      # [batch, output_dim//2]
        file_proj = self.file_projection(file_features)      # [batch, output_dim//2]
        
        # æ‹¼æ¥èåˆ
        fused = torch.cat([line_proj, file_proj], dim=-1)    # [batch, output_dim]
        
        # èåˆå¤„ç†
        output = self.fusion_layer(fused)                    # [batch, output_dim]
        
        return output


class InterLineRelationship(nn.Module):
    """è¡Œé—´å…³ç³»å»ºæ¨¡"""
    
    def __init__(self, feature_dim: int = 128, hidden_dim: int = 64):
        super().__init__()
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        
        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Embedding(1000, hidden_dim)  # æ”¯æŒæœ€å¤š1000è¡Œ
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.self_attention = nn.MultiheadAttention(
            embed_dim=feature_dim + hidden_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(feature_dim + hidden_dim, feature_dim)
    
    def forward(self, line_features: torch.Tensor, line_positions: torch.Tensor) -> torch.Tensor:
        """å»ºæ¨¡è¡Œé—´å…³ç³»"""
        batch_size, seq_len, feature_dim = line_features.shape
        
        # ä½ç½®ç¼–ç 
        pos_embeddings = self.position_embedding(line_positions)  # [batch, seq_len, hidden_dim]
        
        # ç‰¹å¾ä¸ä½ç½®èåˆ
        enhanced_features = torch.cat([line_features, pos_embeddings], dim=-1)  # [batch, seq_len, feature_dim + hidden_dim]
        
        # è‡ªæ³¨æ„åŠ›
        attended_features, _ = self.self_attention(
            enhanced_features, enhanced_features, enhanced_features
        )  # [batch, seq_len, feature_dim + hidden_dim]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(attended_features)  # [batch, seq_len, feature_dim]
        
        return output


class EnhancedAIDetector(nn.Module):
    """å¢å¼ºç‰ˆAIæ£€æµ‹å™¨ - å®Œæ•´æ¶æ„"""
    
    def __init__(self, 
                 codebert_model: str = "microsoft/codebert-base",
                 line_feature_dim: int = 19,
                 file_feature_dim: int = 14,
                 fusion_dim: int = 128,
                 hidden_dim: int = 256):
        super().__init__()
        
        # CodeBERT
        self.tokenizer = AutoTokenizer.from_pretrained(codebert_model)
        self.codebert = AutoModel.from_pretrained(codebert_model)
        self.codebert_dim = self.codebert.config.hidden_size  # 768
        
        # ç‰¹å¾èåˆ
        self.feature_fusion = FeatureFusion(line_feature_dim, file_feature_dim, fusion_dim)
        
        # è¡Œé—´å…³ç³»å»ºæ¨¡
        self.inter_line_relationship = InterLineRelationship(fusion_dim, 64)
        
        # CodeBERTç‰¹å¾æŠ•å½±
        self.codebert_projection = nn.Linear(self.codebert_dim, fusion_dim)
        
        # æœ€ç»ˆç‰¹å¾èåˆ
        self.final_fusion = nn.Sequential(
            nn.Linear(fusion_dim * 2, hidden_dim),  # æ‰‹å·¥ç‰¹å¾ + CodeBERTç‰¹å¾
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim)
        )
        
        # è¡Œåˆ†ç±»å™¨
        self.line_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
    
    def encode_with_codebert(self, lines: List[str]) -> torch.Tensor:
        """ä½¿ç”¨CodeBERTç¼–ç ä»£ç è¡Œ"""
        # æ‰¹é‡å¤„ç†
        processed_lines = []
        for line in lines:
            if not line.strip():
                processed_lines.append("[EMPTY_LINE]")
            else:
                processed_lines.append(line)
        
        # Tokenization
        inputs = self.tokenizer(
            processed_lines,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = next(self.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # CodeBERTç¼–ç 
        with torch.no_grad():
            outputs = self.codebert(**inputs)
        
        # æå–[CLS] tokenè¡¨ç¤º
        code_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch, 768]
        
        return code_embeddings
    
    def forward(self, 
                line_features: torch.Tensor,
                file_features: torch.Tensor,
                code_lines: List[str],
                line_positions: torch.Tensor) -> torch.Tensor:
        """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
        
        # 1. ç‰¹å¾èåˆ
        fused_features = self.feature_fusion(line_features, file_features)  # [batch, fusion_dim]
        
        # 2. è¡Œé—´å…³ç³»å»ºæ¨¡
        if len(fused_features.shape) == 2:
            fused_features = fused_features.unsqueeze(0)  # [1, batch, fusion_dim]
        
        contextual_features = self.inter_line_relationship(fused_features, line_positions)  # [1, batch, fusion_dim]
        contextual_features = contextual_features.squeeze(0)  # [batch, fusion_dim]
        
        # 3. CodeBERTç¼–ç 
        codebert_features = self.encode_with_codebert(code_lines)  # [batch, 768]
        codebert_projected = self.codebert_projection(codebert_features)  # [batch, fusion_dim]
        
        # 4. æœ€ç»ˆç‰¹å¾èåˆ
        final_features = torch.cat([contextual_features, codebert_projected], dim=-1)  # [batch, fusion_dim*2]
        final_features = self.final_fusion(final_features)  # [batch, hidden_dim]
        
        # 5. è¡Œåˆ†ç±»å™¨
        ai_probabilities = self.line_classifier(final_features)  # [batch, 1]
        
        return ai_probabilities.squeeze(-1)  # [batch]


class ThresholdFilter:
    """é˜ˆå€¼è¿‡æ»¤æ¨¡å—"""
    
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
    
    def filter(self, probabilities: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨é˜ˆå€¼è¿‡æ»¤"""
        return (probabilities > self.threshold).float()
    
    def set_threshold(self, threshold: float):
        """åŠ¨æ€è®¾ç½®é˜ˆå€¼"""
        self.threshold = threshold


class ResultAggregator:
    """ç»“æœèšåˆæ¨¡å—"""
    
    def aggregate_file_results(self, 
                             file_info: Dict[str, Any],
                             ai_probabilities: List[float],
                             ai_predictions: List[bool]) -> Dict[str, Any]:
        """èšåˆå•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        lines = file_info['lines']
        
        # é€è¡Œç»“æœ
        line_results = []
        for i, (line_info, ai_prob, is_ai) in enumerate(zip(lines, ai_probabilities, ai_predictions)):
            if not line_info['is_empty']:  # åªå¤„ç†éç©ºè¡Œ
                line_results.append({
                    "line_number": line_info['line_number'],
                    "content": line_info['content'],
                    "ai_prob": round(float(ai_prob), 3),
                    "is_ai": bool(is_ai)
                })
        
        # æ–‡ä»¶çº§ç»Ÿè®¡
        code_lines = [r for r in line_results if r['content'].strip()]
        ai_lines = [r for r in line_results if r['is_ai']]
        
        file_result = {
            "file_path": file_info['filepath'],
            "success": True,
            "lines": line_results,
            "summary": {
                "total_lines": file_info['total_lines'],
                "code_lines": len(code_lines),
                "ai_lines": len(ai_lines),
                "ai_percentage": round((len(ai_lines) / len(code_lines) * 100) if code_lines else 0, 1),
                "average_ai_prob": round(np.mean([r['ai_prob'] for r in code_lines]), 3) if code_lines else 0.0
            }
        }
        
        return file_result
    
    def aggregate_batch_results(self, file_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆæ‰¹é‡æ–‡ä»¶ç»“æœ"""
        successful_results = [r for r in file_results if r.get('success', False)]
        
        # æ€»ä½“ç»Ÿè®¡
        total_files = len(file_results)
        successful_files = len(successful_results)
        failed_files = total_files - successful_files
        
        total_lines = sum(r['summary']['total_lines'] for r in successful_results)
        total_code_lines = sum(r['summary']['code_lines'] for r in successful_results)
        total_ai_lines = sum(r['summary']['ai_lines'] for r in successful_results)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        batch_result = {
            "results": file_results,
            "statistics": {
                "total_files": total_files,
                "successful_files": successful_files,
                "failed_files": failed_files,
                "total_lines": total_lines,
                "total_code_lines": total_code_lines,
                "total_ai_lines": total_ai_lines,
                "overall_ai_percentage": round((total_ai_lines / total_code_lines * 100) if total_code_lines > 0 else 0, 1),
                "average_file_ai_percentage": round(np.mean([r['summary']['ai_percentage'] for r in successful_results]), 1) if successful_results else 0
            },
            "metadata": {
                "model_type": "Enhanced CodeBERT-based AI Detector",
                "architecture": "File Parser â†’ Feature Extraction â†’ Feature Fusion â†’ Inter-line Modeling â†’ CodeBERT â†’ Classifier â†’ Threshold Filter â†’ Result Aggregation",
                "timestamp": datetime.now().isoformat()
            }
        }
        
        return batch_result


class EnhancedAIDetectionSystem:
    """å¢å¼ºç‰ˆAIæ£€æµ‹ç³»ç»Ÿ - å®Œæ•´æµç¨‹"""
    
    def __init__(self, model_path: Optional[str] = None, threshold: float = 0.5):
        self.threshold = threshold
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.file_parser = FileParser()
        self.line_feature_extractor = LineFeatureExtractor()
        self.file_feature_extractor = FileFeatureExtractor()
        self.threshold_filter = ThresholdFilter(threshold)
        self.result_aggregator = ResultAggregator()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = EnhancedAIDetector()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if model_path and os.path.exists(model_path):
            self._load_model(model_path)
        else:
            self.logger.warning("No model file provided or file not found, using random weights")
        
        self.model.eval()
    
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            self.logger.info(f"âœ… Enhanced model loaded from {model_path}")
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
    
    def detect_file(self, filepath: str) -> Dict[str, Any]:
        """æ£€æµ‹å•ä¸ªæ–‡ä»¶"""
        # 1. æ–‡ä»¶è§£æ
        file_info = self.file_parser.parse_file(filepath)
        if not file_info['success']:
            return file_info
        
        # 2. ç‰¹å¾æå–
        line_features_list = []
        code_lines = []
        line_positions = []
        
        for line_info in file_info['lines']:
            if not line_info['is_empty']:  # åªå¤„ç†éç©ºè¡Œ
                # è¡Œçº§ç‰¹å¾
                line_features = self.line_feature_extractor.extract_features(line_info, file_info)
                line_features_list.append(line_features)
                
                # ä»£ç å†…å®¹
                code_lines.append(line_info['content'])
                
                # è¡Œä½ç½®
                line_positions.append(line_info['line_number'] - 1)  # 0-indexed
        
        if not line_features_list:
            return {
                "file_path": filepath,
                "success": False,
                "error": "No code lines found"
            }
        
        # æ–‡ä»¶çº§ç‰¹å¾
        file_features = self.file_feature_extractor.extract_features(file_info)
        
        # 3. è½¬æ¢ä¸ºå¼ é‡
        line_features_tensor = torch.tensor(line_features_list, dtype=torch.float32)
        file_features_tensor = torch.tensor(file_features, dtype=torch.float32).unsqueeze(0).repeat(len(line_features_list), 1)
        line_positions_tensor = torch.tensor(line_positions, dtype=torch.long).unsqueeze(0)
        
        # 4. æ¨¡å‹æ¨ç†
        with torch.no_grad():
            ai_probabilities = self.model(
                line_features_tensor,
                file_features_tensor,
                code_lines,
                line_positions_tensor
            )
        
        # 5. é˜ˆå€¼è¿‡æ»¤
        ai_predictions = self.threshold_filter.filter(ai_probabilities)
        
        # 6. ç»“æœèšåˆ
        # ä¸ºç©ºè¡Œå¡«å……é»˜è®¤å€¼
        full_probabilities = []
        full_predictions = []
        code_idx = 0
        
        for line_info in file_info['lines']:
            if line_info['is_empty']:
                full_probabilities.append(0.0)
                full_predictions.append(False)
            else:
                full_probabilities.append(float(ai_probabilities[code_idx]))
                full_predictions.append(bool(ai_predictions[code_idx]))
                code_idx += 1
        
        result = self.result_aggregator.aggregate_file_results(
            file_info, full_probabilities, full_predictions
        )
        
        return result
    
    def detect_batch(self, input_paths: List[str], recursive: bool = False) -> Dict[str, Any]:
        """æ‰¹é‡æ£€æµ‹"""
        all_files = []
        
        for input_path in input_paths:
            if os.path.isfile(input_path):
                all_files.append(input_path)
            elif os.path.isdir(input_path):
                dir_files = self.file_parser.parse_directory(input_path, recursive)
                all_files.extend([f['filepath'] for f in dir_files if f['success']])
        
        # æ‰¹é‡æ£€æµ‹
        results = []
        for filepath in all_files:
            self.logger.info(f"Processing: {filepath}")
            result = self.detect_file(filepath)
            results.append(result)
        
        # èšåˆç»“æœ
        batch_result = self.result_aggregator.aggregate_batch_results(results)
        
        return batch_result


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description="Enhanced AI Code Detection System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Architecture Flow:
ä»£ç æ–‡ä»¶/æ–‡ä»¶å¤¹ â†’ æ–‡ä»¶è§£æå™¨ â†’ è¡Œçº§ç‰¹å¾æå– â†’ æ–‡ä»¶çº§ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ 
è¡Œé—´å…³ç³»å»ºæ¨¡ â†’ CodeBERTç¼–ç  â†’ æœ€ç»ˆèåˆ â†’ è¡Œåˆ†ç±»å™¨ â†’ é˜ˆå€¼è¿‡æ»¤ â†’ ç»“æœèšåˆ â†’ JSONè¾“å‡º

Examples:
  %(prog)s --input file.py --output results.json
  %(prog)s --input src/ --recursive --threshold 0.7 --output results.json
        """
    )
    
    parser.add_argument("--input", type=str, nargs='+', required=True,
                       help="Input files, directories, or patterns")
    parser.add_argument("--output", type=str, default="detection_results.json",
                       help="Output JSON file")
    parser.add_argument("--model", type=str, default=None,
                       help="Path to trained model file")
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="AI detection threshold (0.0-1.0)")
    parser.add_argument("--recursive", "-r", action="store_true",
                       help="Recursively process directories")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Verbose output")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("ğŸš€ Enhanced AI Code Detection System")
    print("=" * 60)
    print("Architecture: File Parser â†’ Feature Extraction â†’ Feature Fusion â†’")
    print("              Inter-line Modeling â†’ CodeBERT â†’ Classifier â†’")
    print("              Threshold Filter â†’ Result Aggregation")
    print("=" * 60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    detection_system = EnhancedAIDetectionSystem(
        model_path=args.model,
        threshold=args.threshold
    )
    
    # æ‰§è¡Œæ£€æµ‹
    print(f"ğŸ“ Processing: {args.input}")
    print(f"ğŸ¯ Threshold: {args.threshold}")
    print(f"ğŸ”„ Recursive: {args.recursive}")
    
    results = detection_system.detect_batch(args.input, args.recursive)
    
    # ä¿å­˜ç»“æœ
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = results['statistics']
    print(f"\nğŸ“Š Detection Results:")
    print(f"   Files processed: {stats['successful_files']}/{stats['total_files']}")
    print(f"   Total lines: {stats['total_lines']}")
    print(f"   Code lines: {stats['total_code_lines']}")
    print(f"   AI-generated lines: {stats['total_ai_lines']}")
    print(f"   Overall AI percentage: {stats['overall_ai_percentage']}%")
    print(f"\nğŸ’¾ Results saved to: {args.output}")


if __name__ == "__main__":
    main() 