#!/usr/bin/env python3
"""
æ¨¡å—åŒ–AIä»£ç æ£€æµ‹å™¨
æŒ‰ç…§æ¶æ„æµç¨‹å›¾å®ç°çš„å®Œæ•´æ£€æµ‹ç³»ç»Ÿ

æ¶æ„æµç¨‹:
ä»£ç æ–‡ä»¶/æ–‡ä»¶å¤¹ â†’ æ–‡ä»¶è§£æå™¨ â†’ è¡Œçº§ç‰¹å¾æå– â†’ æ–‡ä»¶çº§ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ 
è¡Œé—´å…³ç³»å»ºæ¨¡ â†’ CodeBERTç¼–ç  â†’ è¡Œåˆ†ç±»å™¨ â†’ é˜ˆå€¼è¿‡æ»¤ â†’ ç»“æœèšåˆ â†’ è¾“å‡ºç³»ç»Ÿ
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ‰€æœ‰æ¨¡å—
from modules.file_parser import FileParser
from modules.feature_extraction import LineFeatureExtractor, FileFeatureExtractor
from modules.feature_fusion import FeatureFusion
from modules.inter_line_modeling import InterLineRelationship
from modules.codebert_encoding import CodeBERTEncoder
from modules.classification import LineClassifier
from modules.threshold_filter import ThresholdFilter
from modules.result_aggregation import ResultAggregator
from modules.output_system import OutputSystem

import torch
import torch.nn as nn
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModularAIDetector(nn.Module):
    """æ¨¡å—åŒ–AIæ£€æµ‹å™¨ - å®Œæ•´çš„æ¶æ„å®ç°"""
    
    def __init__(self, 
                 codebert_model: str = "microsoft/codebert-base",
                 threshold: float = 0.5,
                 output_dir: str = "./output"):
        super().__init__()
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—
        self.file_parser = FileParser()
        self.line_feature_extractor = LineFeatureExtractor()
        self.file_feature_extractor = FileFeatureExtractor()
        
        # ç¥ç»ç½‘ç»œæ¨¡å—
        self.feature_fusion = FeatureFusion(line_feature_dim=19, file_feature_dim=14, output_dim=128)
        self.inter_line_modeling = InterLineRelationship(feature_dim=128, hidden_dim=64)
        self.codebert_encoder = CodeBERTEncoder(model_name=codebert_model, output_dim=128)
        self.line_classifier = LineClassifier(input_dim=256, hidden_dims=[256, 128, 64])
        
        # åå¤„ç†æ¨¡å—
        self.threshold_filter = ThresholdFilter(threshold=threshold)
        self.result_aggregator = ResultAggregator()
        self.output_system = OutputSystem(output_dir=output_dir)
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.eval()
    
    def detect_file(self, filepath: str) -> Dict[str, Any]:
        """
        æ£€æµ‹å•ä¸ªæ–‡ä»¶
        æŒ‰ç…§å®Œæ•´çš„æ¶æ„æµç¨‹å¤„ç†
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        logger.info(f"ğŸ” å¼€å§‹æ£€æµ‹æ–‡ä»¶: {filepath}")
        
        # æ­¥éª¤1: æ–‡ä»¶è§£æ
        logger.debug("ğŸ“„ æ­¥éª¤1: æ–‡ä»¶è§£æ")
        file_info = self.file_parser.parse_file(filepath)
        if not file_info['success']:
            return file_info
        
        # ç­›é€‰éç©ºè¡Œ
        non_empty_lines = [line for line in file_info['lines'] if not line['is_empty']]
        if not non_empty_lines:
            return {
                "file_path": filepath,
                "success": False,
                "error": "No code lines found"
            }
        
        # æ­¥éª¤2: è¡Œçº§ç‰¹å¾æå–
        logger.debug("ğŸ”§ æ­¥éª¤2: è¡Œçº§ç‰¹å¾æå–")
        line_features_list = []
        for line_info in non_empty_lines:
            features = self.line_feature_extractor.extract_features(line_info, file_info)
            line_features_list.append(features)
        
        # æ­¥éª¤3: æ–‡ä»¶çº§ç‰¹å¾æå–
        logger.debug("ğŸ“Š æ­¥éª¤3: æ–‡ä»¶çº§ç‰¹å¾æå–")
        file_features = self.file_feature_extractor.extract_features(file_info)
        
        # è½¬æ¢ä¸ºå¼ é‡
        line_features_tensor = torch.tensor(line_features_list, dtype=torch.float32)
        file_features_tensor = torch.tensor(file_features, dtype=torch.float32).unsqueeze(0).repeat(len(line_features_list), 1)
        
        # æ­¥éª¤4: ç‰¹å¾èåˆ
        logger.debug("ğŸ”— æ­¥éª¤4: ç‰¹å¾èåˆ")
        fused_features = self.feature_fusion(line_features_tensor, file_features_tensor)
        
        # æ­¥éª¤5: è¡Œé—´å…³ç³»å»ºæ¨¡
        logger.debug("ğŸ§  æ­¥éª¤5: è¡Œé—´å…³ç³»å»ºæ¨¡")
        line_positions = torch.tensor([line['line_number'] - 1 for line in non_empty_lines], dtype=torch.long).unsqueeze(0)
        fused_features_expanded = fused_features.unsqueeze(0)  # [1, num_lines, feature_dim]
        contextual_features = self.inter_line_modeling(fused_features_expanded, line_positions)
        contextual_features = contextual_features.squeeze(0)  # [num_lines, feature_dim]
        
        # æ­¥éª¤6: CodeBERTç¼–ç 
        logger.debug("ğŸ¤– æ­¥éª¤6: CodeBERTç¼–ç ")
        code_lines = [line['content'] for line in non_empty_lines]
        codebert_features = self.codebert_encoder.encode_lines(code_lines)
        
        # æœ€ç»ˆç‰¹å¾èåˆ
        final_features = torch.cat([contextual_features, codebert_features], dim=-1)
        
        # æ­¥éª¤7: è¡Œåˆ†ç±»å™¨
        logger.debug("ğŸ¯ æ­¥éª¤7: è¡Œåˆ†ç±»å™¨é¢„æµ‹")
        with torch.no_grad():
            ai_probabilities = self.line_classifier(final_features)
        
        # æ­¥éª¤8: é˜ˆå€¼è¿‡æ»¤
        logger.debug("âš–ï¸ æ­¥éª¤8: é˜ˆå€¼è¿‡æ»¤")
        ai_predictions = self.threshold_filter.filter(ai_probabilities)
        
        # ä¸ºæ‰€æœ‰è¡Œï¼ˆåŒ…æ‹¬ç©ºè¡Œï¼‰å¡«å……ç»“æœ
        full_probabilities = []
        full_predictions = []
        code_idx = 0
        
        for line_info in file_info['lines']:
            if line_info['is_empty']:
                full_probabilities.append(0.0)
                full_predictions.append(False)
            else:
                full_probabilities.append(float(ai_probabilities[code_idx]))
                full_predictions.append(bool(ai_predictions[code_idx]))
                code_idx += 1
        
        # æ­¥éª¤9: ç»“æœèšåˆ
        logger.debug("ğŸ“‹ æ­¥éª¤9: ç»“æœèšåˆ")
        result = self.result_aggregator.aggregate_file_results(
            file_info, full_probabilities, full_predictions
        )
        
        logger.info(f"âœ… æ–‡ä»¶æ£€æµ‹å®Œæˆ: AIæ¯”ä¾‹ {result['summary']['ai_percentage']}%")
        return result
    
    def detect_batch(self, input_paths: List[str], recursive: bool = False) -> Dict[str, Any]:
        """
        æ‰¹é‡æ£€æµ‹
        
        Args:
            input_paths: è¾“å…¥è·¯å¾„åˆ—è¡¨
            recursive: æ˜¯å¦é€’å½’å¤„ç†ç›®å½•
            
        Returns:
            Dict: æ‰¹é‡æ£€æµ‹ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹: {len(input_paths)} ä¸ªè·¯å¾„")
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
        all_files = []
        for input_path in input_paths:
            if os.path.isfile(input_path):
                if self.file_parser.is_code_file(os.path.basename(input_path)):
                    all_files.append(input_path)
            elif os.path.isdir(input_path):
                dir_files = self.file_parser.parse_directory(input_path, recursive)
                all_files.extend([f['filepath'] for f in dir_files if f['success']])
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(all_files)} ä¸ªä»£ç æ–‡ä»¶")
        
        # æ‰¹é‡æ£€æµ‹
        results = []
        for i, filepath in enumerate(all_files, 1):
            logger.info(f"ğŸ“ å¤„ç† {i}/{len(all_files)}: {os.path.basename(filepath)}")
            result = self.detect_file(filepath)
            results.append(result)
        
        # æ­¥éª¤9: ç»“æœèšåˆï¼ˆæ‰¹é‡ï¼‰
        logger.info("ğŸ“Š èšåˆæ‰¹é‡ç»“æœ")
        batch_result = self.result_aggregator.aggregate_batch_results(results)
        
        return batch_result
    
    def detect_and_output(self, 
                         input_paths: List[str],
                         output_formats: List[str] = ['json'],
                         output_filename: str = None,
                         recursive: bool = False) -> Dict[str, str]:
        """
        æ£€æµ‹å¹¶è¾“å‡ºç»“æœ
        
        Args:
            input_paths: è¾“å…¥è·¯å¾„åˆ—è¡¨
            output_formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨
            output_filename: è¾“å‡ºæ–‡ä»¶å
            recursive: æ˜¯å¦é€’å½’å¤„ç†
            
        Returns:
            Dict: è¾“å‡ºæ–‡ä»¶è·¯å¾„æ˜ å°„
        """
        # æ‰§è¡Œæ£€æµ‹
        results = self.detect_batch(input_paths, recursive)
        
        # æ­¥éª¤10: è¾“å‡ºç³»ç»Ÿ
        logger.info("ğŸ’¾ æ­¥éª¤10: è¾“å‡ºç»“æœ")
        output_files = self.output_system.output_multiple_formats(
            results, output_formats, output_filename
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = results['statistics']
        logger.info("ğŸ“ˆ æ£€æµ‹ç»Ÿè®¡:")
        logger.info(f"   æ–‡ä»¶æ€»æ•°: {stats['total_files']}")
        logger.info(f"   æˆåŠŸå¤„ç†: {stats['successful_files']}")
        logger.info(f"   ä»£ç è¡Œæ•°: {stats['total_code_lines']}")
        logger.info(f"   AIè¡Œæ•°: {stats['total_ai_lines']}")
        logger.info(f"   AIæ¯”ä¾‹: {stats['overall_ai_percentage']}%")
        
        return output_files
    
    def set_threshold(self, threshold: float):
        """è®¾ç½®æ£€æµ‹é˜ˆå€¼"""
        self.threshold_filter.set_threshold(threshold)
    
    def get_architecture_info(self) -> Dict[str, Any]:
        """è·å–æ¶æ„ä¿¡æ¯"""
        return {
            "architecture_flow": [
                "æ–‡ä»¶è§£æå™¨",
                "è¡Œçº§ç‰¹å¾æå–",
                "æ–‡ä»¶çº§ç‰¹å¾æå–", 
                "ç‰¹å¾èåˆ",
                "è¡Œé—´å…³ç³»å»ºæ¨¡",
                "CodeBERTç¼–ç ",
                "è¡Œåˆ†ç±»å™¨",
                "é˜ˆå€¼è¿‡æ»¤",
                "ç»“æœèšåˆ",
                "è¾“å‡ºç³»ç»Ÿ"
            ],
            "modules": {
                "file_parser": "FileParser",
                "line_feature_extractor": "LineFeatureExtractor (19ç»´ç‰¹å¾)",
                "file_feature_extractor": "FileFeatureExtractor (14ç»´ç‰¹å¾)",
                "feature_fusion": "FeatureFusion (19+14 â†’ 128)",
                "inter_line_modeling": "InterLineRelationship (è‡ªæ³¨æ„åŠ›)",
                "codebert_encoder": "CodeBERTEncoder (microsoft/codebert-base)",
                "line_classifier": "LineClassifier (256 â†’ 1)",
                "threshold_filter": f"ThresholdFilter (é˜ˆå€¼: {self.threshold_filter.get_threshold()})",
                "result_aggregator": "ResultAggregator",
                "output_system": "OutputSystem (å¤šæ ¼å¼è¾“å‡º)"
            },
            "supported_languages": list(self.file_parser.supported_extensions),
            "output_formats": self.output_system.get_supported_formats()
        }


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description="æ¨¡å—åŒ–AIä»£ç æ£€æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ¶æ„æµç¨‹:
ä»£ç æ–‡ä»¶/æ–‡ä»¶å¤¹ â†’ æ–‡ä»¶è§£æå™¨ â†’ è¡Œçº§ç‰¹å¾æå– â†’ æ–‡ä»¶çº§ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ 
è¡Œé—´å…³ç³»å»ºæ¨¡ â†’ CodeBERTç¼–ç  â†’ è¡Œåˆ†ç±»å™¨ â†’ é˜ˆå€¼è¿‡æ»¤ â†’ ç»“æœèšåˆ â†’ è¾“å‡ºç³»ç»Ÿ

ç¤ºä¾‹:
  %(prog)s --input file.py --output results
  %(prog)s --input src/ --recursive --formats json csv html
  %(prog)s --input *.py --threshold 0.7 --output-dir ./reports
        """
    )
    
    parser.add_argument("--input", type=str, nargs='+',
                       help="è¾“å…¥æ–‡ä»¶ã€ç›®å½•æˆ–æ¨¡å¼")
    parser.add_argument("--output", type=str, default="ai_detection_results",
                       help="è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰")
    parser.add_argument("--output-dir", type=str, default="./output",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--formats", type=str, nargs='+', 
                       default=['json'], choices=['json', 'csv', 'xml', 'txt', 'html'],
                       help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="AIæ£€æµ‹é˜ˆå€¼ (0.0-1.0)")
    parser.add_argument("--recursive", "-r", action="store_true",
                       help="é€’å½’å¤„ç†ç›®å½•")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--info", action="store_true",
                       help="æ˜¾ç¤ºæ¶æ„ä¿¡æ¯")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("ğŸš€ æ¨¡å—åŒ–AIä»£ç æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 80)
    print("æ¶æ„æµç¨‹: æ–‡ä»¶è§£æå™¨ â†’ ç‰¹å¾æå– â†’ ç‰¹å¾èåˆ â†’ è¡Œé—´å»ºæ¨¡ â†’ CodeBERT â†’ åˆ†ç±»å™¨ â†’ è¿‡æ»¤ â†’ èšåˆ â†’ è¾“å‡º")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = ModularAIDetector(threshold=args.threshold, output_dir=args.output_dir)
    
    # æ˜¾ç¤ºæ¶æ„ä¿¡æ¯
    if args.info:
        arch_info = detector.get_architecture_info()
        print("\nğŸ“‹ æ¶æ„ä¿¡æ¯:")
        print("æµç¨‹æ­¥éª¤:")
        for i, step in enumerate(arch_info['architecture_flow'], 1):
            print(f"  {i}. {step}")
        print("\næ¨¡å—è¯¦æƒ…:")
        for name, desc in arch_info['modules'].items():
            print(f"  {name}: {desc}")
        print(f"\næ”¯æŒè¯­è¨€: {', '.join(arch_info['supported_languages'])}")
        print(f"è¾“å‡ºæ ¼å¼: {', '.join(arch_info['output_formats'])}")
        return
    
    # æ£€æŸ¥å¿…éœ€çš„å‚æ•°
    if not args.input:
        parser.error("--input æ˜¯å¿…éœ€çš„å‚æ•°ï¼ˆé™¤éä½¿ç”¨ --infoï¼‰")
    
    # æ‰§è¡Œæ£€æµ‹
    print(f"\nğŸ“ è¾“å…¥: {args.input}")
    print(f"ğŸ¯ é˜ˆå€¼: {args.threshold}")
    print(f"ğŸ“Š æ ¼å¼: {', '.join(args.formats)}")
    print(f"ğŸ”„ é€’å½’: {args.recursive}")
    
    try:
        output_files = detector.detect_and_output(
            args.input, 
            args.formats, 
            args.output,
            args.recursive
        )
        
        print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        for format_type, filepath in output_files.items():
            if not filepath.startswith("Error:"):
                print(f"  {format_type}: {filepath}")
            else:
                print(f"  {format_type}: âŒ {filepath}")
        
        print(f"\nâœ¨ æ£€æµ‹å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        if args.verbose:
            traceback.print_exc()


if __name__ == "__main__":
    main() 