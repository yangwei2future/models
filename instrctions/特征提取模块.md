最终特征提取系统开发任务描述
任务目标
开发端到端特征提取管道，将文件级和行级特征处理为CodeBERT兼容的输入格式，用于代码来源检测模型

系统架构设计
文件级特征提取 (CodeBERT输入准备)
1. 语言识别 (Language Identification)
python
def detect_language(file_path: str, content: str) -> str:
    """
    返回标准语言标识符 (python/java/javascript/cpp)
    输出示例: "python"
    用途: 确定CodeBERT的tokenizer类型
    """
2. 导入分析 (Import Analysis)
python
def analyze_imports(content: str, language: str) -> list:
    """
    返回标准化的依赖库列表
    输出示例: ["numpy", "pandas", "os"]
    用途: 构建CodeBERT的库上下文提示
    """
3. 文件元数据 (File Metadata)
python
def extract_metadata(file_path: str) -> dict:
    """
    返回关键元数据:
        {
          "line_count": int,
          "indent_type": "spaces/tabs",
          "encoding": "utf-8"
        }
    用途: 指导特征归一化处理
    """
4. 文件上下文编码器 (File Context Encoder)
python
class FileEncoder:
    def __init__(self, model_name="microsoft/codebert-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
    
    def encode(self, content: str) -> dict:
        """
        返回CodeBERT兼容的编码:
            {
                "input_ids": tensor,
                "attention_mask": tensor,
                "file_embedding": tensor  # [CLS]向量
            }
        处理流程:
            1. 智能分块 (max_length=1024)
            2. 添加特殊token: [FILE_START][LANG=python][IMPORTS=numpy,os][FILE_END]
            3. 生成各块的嵌入表示
        """
行级特征提取 (CodeBERT增强输入)
1. 文本特征提取器
python
class LineTextFeature:
    def extract(self, line: str) -> list:
        """
        返回归一化数值特征:
            [length_norm, operator_density, entropy, is_comment]
        维度: 4维向量
        """
2. AST结构提取器
python
class LineASTFeature:
    def extract(self, line: str, ast_tree: object) -> list:
        """
        返回AST特征:
            [depth_norm, node_type_id, children_count, is_control_flow]
        维度: 4维向量
        """
3. 代码风格提取器
python
class LineStyleFeature:
    def extract(self, line: str, metadata: dict) -> list:
        """
        返回风格特征:
            [indent_diff, naming_score, brace_style]
        维度: 3维向量
        """
4. 上下文窗口编码器
python
class ContextEncoder:
    def encode(self, lines: list, current_idx: int) -> list:
        """
        返回上下文向量 (滑动窗口):
            [line-2, line-1, current_line, line+1, line+2]
        处理: 
            1. 位置编码嵌入
            2. 返回各行的token ID序列
        维度: 5*128=640维 (128维/行)
        """
特征融合与CodeBERT输入构造
特征融合器
python
class FeatureFuser:
    def fuse_line_features(self, text_feat, ast_feat, style_feat) -> torch.Tensor:
        """
        拼接行级特征为单一张量:
            [文本特征(4) + AST(4) + 风格(3)] = 11维
        返回: (1, 11) 形状张量
        """

    def add_context_to_bert(self, bert_input, context_vectors):
        """
        将上下文向量注入BERT输入:
        1. 创建新的位置编码层
        2. 修改attention_mask包含上下文
        3. 返回增强的BERT输入字典
        """
CodeBERT输入构造器
python
class CodeBERTInputBuilder:
    def build(self, file_embedding, line_features, context_vectors) -> dict:
        """
        构造最终模型输入:
        {
            "file_embedding": tensor,    # 文件级上下文 [1, 768]
            "line_features": tensor,     # 行级特征矩阵 [n_lines, 11]
            "context_vectors": tensor,   # 上下文窗口 [n_lines, 5, 128]
            "attention_mask": tensor     # 自定义注意力掩码
        }
        特殊处理:
            - 行特征与文件嵌入的维度对齐
            - 动态注意力权重分配
        """
端到端处理管道
python
class FeatureExtractionPipeline:
    def __init__(self, codebert_model="microsoft/codebert-base"):
        self.file_encoder = FileEncoder(codebert_model)
        self.lang_detector = LanguageDetector()
        self.import_analyzer = ImportAnalyzer()
        self.metadata_extractor = MetadataExtractor()
        
    def process(self, file_path: str) -> dict:
        # 读取文件内容
        content = self._read_file(file_path)
        
        # 文件级处理
        lang = self.lang_detector(file_path, content)
        imports = self.import_analyzer(content, lang)
        metadata = self.metadata_extractor(file_path)
        file_embedding = self.file_encoder.encode(content)
        
        # 准备行级处理
        lines = content.split('\n')
        ast_tree = self._parse_ast(content, lang)
        
        # 行级特征提取
        line_features = []
        context_vectors = []
        context_encoder = ContextEncoder()
        
        for idx, line in enumerate(lines):
            # 跳过空行
            if not line.strip(): continue
            
            # 提取特征
            text_feat = LineTextFeature().extract(line)
            ast_feat = LineASTFeature().extract(line, ast_tree)
            style_feat = LineStyleFeature().extract(line, metadata)
            
            # 融合基础特征
            fused = FeatureFuser().fuse_line_features(text_feat, ast_feat, style_feat)
            line_features.append(fused)
            
            # 上下文编码
            context_vec = context_encoder.encode(lines, idx)
            context_vectors.append(context_vec)
        
        # 构造CodeBERT输入
        model_input = CodeBERTInputBuilder().build(
            file_embedding['file_embedding'],
            torch.stack(line_features),
            torch.stack(context_vectors)
        )
        
        return {
            "model_input": model_input,
            "metadata": {
                "language": lang,
                "line_count": len(lines),
                "imports": imports
            }
        }
CodeBERT输入规范
输入数据结构
python
{
    "file_embedding": torch.Tensor,  # 形状 [1, 768]
    "line_features": torch.Tensor,   # 形状 [num_lines, 11]
    "context_vectors": torch.Tensor, # 形状 [num_lines, 5, 128]
    "attention_mask": torch.Tensor   # 形状 [num_lines, 5]
}
特征维度说明
特征类型	维度	来源
文件嵌入	768	CodeBERT [CLS]向量
文本特征	4	行长度/操作符密度/熵/注释标记
AST特征	4	深度/节点类型/子节点数/控制流标记
风格特征	3	缩进差异/命名规范/括号风格
上下文窗口	5×128	周围5行的编码
开发要求
性能优化
AST共享解析

python
# 全局AST缓存
_AST_CACHE = {}

def get_ast(content, lang):
    key = hash(content)
    if key not in _AST_CACHE:
        _AST_CACHE[key] = parse_ast(content, lang)
    return _AST_CACHE[key]
批处理加速

python
# 行特征批量提取
def batch_extract(lines, extractor):
    return [extractor(line) for line in lines]
内存映射大文件

python
# 处理超大文件
with open(file_path, "r", encoding="utf-8") as f:
    for chunk in read_in_chunks(f, size=1000):
        process_chunk(chunk)
测试规范
输入验证测试

python
def test_codebert_input_shape():
    input = pipeline.process("sample.py")["model_input"]
    assert input["line_features"].shape[1] == 11
    assert input["context_vectors"].dim() == 3
多语言覆盖测试

python
@pytest.mark.parametrize("lang", ["python", "java", "javascript"])
def test_language_support(lang):
    test_file = generate_test_file(lang)
    result = pipeline.process(test_file)
    assert "model_input" in result
性能基准

文件大小	最大延迟	GPU内存
<100行	50ms	<500MB
1000行	200ms	<1GB
10,000行	1.5s	<2GB
交付物
代码结构
text
src/
├── file_features/
│   ├── language.py
│   ├── imports.py
│   ├── metadata.py
│   └── encoder.py
├── line_features/
│   ├── text.py
│   ├── ast.py
│   ├── style.py
│   └── context.py
├── fusion/
│   ├── feature_fuser.py
│   └── input_builder.py
├── pipeline.py
└── utils/
    ├── ast_parser.py
    └── memory.py
接口规范
python
# 使用示例
from src.pipeline import FeatureExtractionPipeline

pipeline = FeatureExtractionPipeline()
result = pipeline.process("source_code.py")

# 输出格式
{
    "model_input": {
        "file_embedding": tensor,    # 文件级表示
        "line_features": tensor,     # 行级特征矩阵
        "context_vectors": tensor,   # 上下文编码
        "attention_mask": tensor     # 自定义注意力
    },
    "metadata": {...}                # 辅助信息
}
该设计将文件级特征浓缩为768维全局表示，行级特征处理为紧凑的11维向量，通过上下文窗口保持行间关系，最终形成CodeBERT友好的输入结构。特征提取过程完全面向模型服务，避免不必要的中间存储，在保持高精度的同时最小化数据处理开销。